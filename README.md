# community_citation_model
This is a Python repository implementing Community Citation Model.

# Reproducing the results

The code to generate the results is available in the [`repro`](./repro) directory. All scripts are written in Python and can be run with Snakemake.

## Installing the package for reproducing the results

In order to install the package for reproducing the results, run the following command:

```bash
cd repro
conda env create -f environment.yml
```

This will create a conda environment named `citationdynamics` with the packages required to run the scripts.
Then, activate the environment with the following command:

```bash
conda activate citationdynamics
```

We will need some custom libraries to run the workflows. These custom libraries are under `./repo/libs/` folders. Please install them by running the following command:
```bash
pip install -e ./repo/libs/geocitmodel
pip install -e ./repo/libs/xnet
```

## Running the scripts

We built the workflows with [Snakemake](https://snakemake.readthedocs.io/en/stable/), a workflow management system.


### Requirements

- Python 3.9 or later
- GPU memory >= 16GB
- CUDA >= 11.0
- Memory: > 500GB
- Disk space >= 2T

Typical installation time for normal desktop computer is less than one hour.

### Set up the environment variables
First, set the environment specific variables in the [`repro/workflow/config.yaml`](./repro/workflow/config.yaml) file.
The variables defined in the [`repro/workflow/config.yaml`](./repro/workflow/config.yaml) file are
- `data_dir`: The directory where the raw data is stored.
- `supp_data_dir`: The directory where the supplementary data is stored (which is [repro/data_suppl](./repro/data_suppl)).
- `aps_data_dir`: The directory where the APS data is stored.
- `legcit_data_dir`: The directory where the Case Law data is stored.
- `uspto_data_dir`: The directory where the USPTO data is stored.

Please download the raw data from the following links and put them in the the directory you specified in the `config.yaml` file.
- APS: https://journals.aps.org/datasets
- Case Law: https://case.law/
- USPTO: https://patentsview.org/

Alternatively, you can download the preprocessed data from the following link and put them in the `<data_dir>/<data name>/preprocessed` directory.
The `<data name>` must be one of the following: aps (for the APS), legcitv2 (for the Case Law), uspto (for the USPTO).
Due to the proprietary nature of the APS data, we cannot provide the preprocessed data.

Figshare: https://figshare.com/account/projects/202335/articles/25655514

### Data preprocessing

** Skip this step if you have downloaded the preprocessed the data.**

A data preprocessing script is provided in workflow/<data name>/Snakefile. Go to the workflow/<data name> directory and run the following command to preprocess the data:

```bash
snakemake --cores <number of cores>
```

If you want to run the workflows for a custom dataset, you need to generate the following four files and put them under the `data/<data name>/preprocessed` directory:
- `paper_table.csv`: a csv file consisting of columns year,group,eta,paper_id
- `citation_table.npz`: the adjacency matrix of the citation network in form of `scipy.sparse.csr_matrix`. The rows and column represent the citing and cited papers, respectively. The matrix is assumed to be `scipy.sparse.csr_matrix` and the file is generated by `scipy.sparse.save_npz` api.
- `paper_category_table.csv`: a csv file consisting of columns paper_id,sub_class_id,main_class_id,sequence. paper_id is the paper id in the paper_table.csv. sub_class_id is the sub category id in the category_table.csv. main_class_id is the main category id in the category_table.csv. sequence is the order of the keyword in the paper. If the paper has one keyword, the sequence is 0.
- `category_table.csv`: a csv file consisting of columns group,title,class_id,category_id. group is the group name. title is the category name. class_id is the class id. category_id is the category id.

A demo data is provided under `data/demo/preprocessed` folder.

### Running the workflows

Run the following command to run the workflows:

```bash
snakemake --cores <number of cores>
```
This will generate the results in the [`repro/results`](./repro/results) directory.


### Expected outcomes

All figures in the paper are generated by the scripts and saved in `figs` folder. The figures are generated by intermediate files in `<data name>/plot_data` directory.
The expected run time for the normal desktop computer varies depending on the availability of the GPU and the number of cores.
For reference, with four GPU cards and 10 CPU cores, the workflow for the demo data takes about 500 mins.


# Installing the package for the CCMs
TBD

# TODOs

- [ ] CCM
  - [x] Set up repo
  - [x] Write test script
  - [ ] Move the code and remove unused functions
  - [ ] Rename variables and functions
  - [ ] Write the installation instruction on README
  - [ ] Test
- [ ] Automatic workflow for reproduceability
  - [x] Move Snakefile and workflow scripts
  - [x] Run the workflow and make sure that it works
  - [x] Write the instruction on README.
  - [ ] Remove unused rules & variables
  - [ ] Pack all rules into one Snakemake
  - [ ] Replace old CCM code with new CCM code
